{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import jieba\n",
    "import langconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'D:\\\\python\\\\datasource\\\\Lession_10\\\\movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie = pd.read_csv(fname, encoding = 'utf8')\n",
    "len(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "link\n",
      "name\n",
      "comment\n",
      "star\n"
     ]
    }
   ],
   "source": [
    "for t in movie:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 lines\n",
      "Saved 2000 lines\n",
      "Saved 4000 lines\n",
      "Saved 6000 lines\n",
      "Saved 8000 lines\n",
      "Saved 10000 lines\n",
      "Saved 12000 lines\n",
      "Saved 14000 lines\n",
      "Saved 16000 lines\n",
      "Saved 18000 lines\n",
      "Saved 20000 lines\n",
      "Saved 22000 lines\n",
      "Saved 24000 lines\n",
      "Saved 26000 lines\n",
      "Saved 28000 lines\n",
      "Saved 30000 lines\n",
      "Saved 32000 lines\n",
      "Saved 34000 lines\n",
      "Saved 36000 lines\n",
      "Saved 38000 lines\n",
      "Saved 40000 lines\n",
      "Saved 42000 lines\n",
      "Saved 44000 lines\n",
      "Saved 46000 lines\n",
      "Saved 48000 lines\n",
      "Saved 50000 lines\n",
      "Saved 52000 lines\n",
      "Saved 54000 lines\n",
      "Saved 56000 lines\n",
      "Saved 58000 lines\n",
      "Saved 60000 lines\n",
      "Saved 62000 lines\n",
      "Saved 64000 lines\n",
      "Saved 66000 lines\n",
      "Saved 68000 lines\n",
      "Saved 70000 lines\n",
      "Saved 72000 lines\n",
      "Saved 74000 lines\n",
      "Saved 76000 lines\n",
      "Saved 78000 lines\n",
      "Saved 80000 lines\n",
      "Saved 82000 lines\n",
      "Saved 84000 lines\n",
      "Saved 86000 lines\n",
      "Saved 88000 lines\n",
      "Saved 90000 lines\n",
      "Saved 92000 lines\n",
      "Saved 94000 lines\n",
      "Saved 96000 lines\n",
      "Saved 98000 lines\n",
      "Saved 100000 lines\n",
      "Saved 102000 lines\n",
      "Saved 104000 lines\n",
      "Saved 106000 lines\n",
      "Saved 108000 lines\n",
      "Saved 110000 lines\n",
      "Saved 112000 lines\n",
      "Saved 114000 lines\n",
      "Saved 116000 lines\n",
      "Saved 118000 lines\n",
      "Saved 120000 lines\n",
      "Saved 122000 lines\n",
      "Saved 124000 lines\n",
      "Saved 126000 lines\n",
      "Saved 128000 lines\n",
      "Saved 130000 lines\n",
      "Saved 132000 lines\n",
      "Saved 134000 lines\n",
      "Saved 136000 lines\n",
      "Saved 138000 lines\n",
      "Saved 140000 lines\n",
      "Saved 142000 lines\n",
      "Saved 144000 lines\n",
      "Saved 146000 lines\n",
      "Saved 148000 lines\n",
      "Saved 150000 lines\n",
      "Saved 152000 lines\n",
      "Saved 154000 lines\n",
      "Saved 156000 lines\n",
      "Saved 158000 lines\n",
      "Saved 160000 lines\n",
      "Saved 162000 lines\n",
      "Saved 164000 lines\n",
      "Saved 166000 lines\n",
      "Saved 168000 lines\n",
      "Saved 170000 lines\n",
      "Saved 172000 lines\n",
      "Saved 174000 lines\n",
      "Saved 176000 lines\n",
      "Saved 178000 lines\n",
      "Saved 180000 lines\n",
      "Saved 182000 lines\n",
      "Saved 184000 lines\n",
      "Saved 186000 lines\n",
      "Saved 188000 lines\n",
      "Saved 190000 lines\n",
      "Saved 192000 lines\n",
      "Saved 194000 lines\n",
      "Saved 196000 lines\n",
      "Saved 198000 lines\n",
      "Saved 200000 lines\n",
      "Saved 202000 lines\n",
      "Saved 204000 lines\n",
      "Saved 206000 lines\n",
      "Saved 208000 lines\n",
      "Saved 210000 lines\n",
      "Saved 212000 lines\n",
      "Saved 214000 lines\n",
      "Saved 216000 lines\n",
      "Saved 218000 lines\n",
      "Saved 220000 lines\n",
      "Saved 222000 lines\n",
      "Saved 224000 lines\n",
      "Saved 226000 lines\n",
      "Saved 228000 lines\n",
      "Saved 230000 lines\n",
      "Saved 232000 lines\n",
      "Saved 234000 lines\n",
      "Saved 236000 lines\n",
      "Saved 238000 lines\n",
      "Saved 240000 lines\n",
      "Saved 242000 lines\n",
      "Saved 244000 lines\n",
      "Saved 246000 lines\n",
      "Saved 248000 lines\n",
      "Saved 250000 lines\n",
      "Saved 252000 lines\n",
      "Saved 254000 lines\n",
      "Saved 256000 lines\n",
      "Saved 258000 lines\n",
      "Saved 260000 lines\n"
     ]
    }
   ],
   "source": [
    "comment_id = []\n",
    "comment_comment = []\n",
    "comment_star = []\n",
    "for i, content in enumerate(movie['comment']):    \n",
    "    comment, star = movie['comment'][i], movie['star'][i]\n",
    "    star = str(star)\n",
    "    if not star.isdigit(): continue\n",
    "    text = str(comment)\n",
    "    text = text.strip()\n",
    "    if not text: continue\n",
    "    text = langconv.Converter('zh-hans').convert(text)\n",
    "    text = ''.join(re.findall(r'[\\d|\\w]+',text))\n",
    "    text = ' '.join(jieba.cut(text))\n",
    "    comment_id.append(id)    \n",
    "    comment_comment.append(text)\n",
    "    comment_star.append(star)\n",
    "    if i % 2000 ==0:\n",
    "        print('Saved ' + str(i) + ' lines')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutFile = 'D:\\python\\datasource\\Lession_10\\cutResult.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'id':comment_id, 'comment':comment_comment, 'star':comment_star})\n",
    "data.to_csv(cutFile,encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment\n",
      "id\n",
      "star\n"
     ]
    }
   ],
   "source": [
    "cut_comment = pd.read_csv(cutFile, encoding = 'gb18030')\n",
    "cut_comment = cut_comment.loc[:, ~cut_comment.columns.str.contains('^Unnamed')]\n",
    "for c in cut_comment:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_comment = cut_comment.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261496"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cut_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>id</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>吴京 意淫 到 了 脑残 的 地步 看 了 恶心 想 吐</td>\n",
       "      <td>260149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>首映礼 看 的 太 恐怖 了 这个 电影 不讲道理 的 完全 就是 吴京 在 实现 他 这个...</td>\n",
       "      <td>260149</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>吴京 的 炒作 水平 不输 冯小刚 但小刚 至少 不会 用 主旋律 来 炒作 吴京 让 人 ...</td>\n",
       "      <td>260149</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>凭良心说 好 看到 不像 战狼 1 的 续集 完虐 湄公河 行动</td>\n",
       "      <td>260149</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>中二得 很</td>\n",
       "      <td>260149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment      id  star\n",
       "0                       吴京 意淫 到 了 脑残 的 地步 看 了 恶心 想 吐  260149     1\n",
       "1  首映礼 看 的 太 恐怖 了 这个 电影 不讲道理 的 完全 就是 吴京 在 实现 他 这个...  260149     2\n",
       "2  吴京 的 炒作 水平 不输 冯小刚 但小刚 至少 不会 用 主旋律 来 炒作 吴京 让 人 ...  260149     2\n",
       "3                   凭良心说 好 看到 不像 战狼 1 的 续集 完虐 湄公河 行动  260149     4\n",
       "4                                              中二得 很  260149     1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_class = []\n",
    "for i in cut_comment['star']:\n",
    "    i = str(i)\n",
    "    star = int(i)\n",
    "    if star > 3:\n",
    "        comment_class.append([1, 0])\n",
    "    else:\n",
    "        comment_class.append([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261496"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comment_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练集/测试集/验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(range(len(comment_class)),size = 12000, replace = False).tolist()\n",
    "train_index = index[0:5000]\n",
    "test_index = index[5000:10000]\n",
    "valid_index = index[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [cut_comment['comment'][i] for i in train_index]\n",
    "train_y = [comment_class[i] for i in train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [cut_comment['comment'][i] for i in test_index]\n",
    "test_y = [comment_class[i] for i in test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_text = [cut_comment['comment'][i] for i in valid_index]\n",
    "valid_y = [comment_class[i] for i in valid_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 10000\n",
    "text = train_text\n",
    "tfidf = TfidfVectorizer(max_features=feature_size, stop_words=stop_words)\n",
    "fit_tfidf = tfidf.fit_transform(text)\n",
    "train_x = fit_tfidf.toarray()\n",
    "test_x = tfidf.transform(test_text).toarray()\n",
    "valid_x = tfidf.transform(valid_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 10.296930\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 54.4%\n",
      "Minibatch loss at step 50: 7.134218\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 55.0%\n",
      "Minibatch loss at step 100: 1.444922\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 56.6%\n",
      "Minibatch loss at step 150: 1.472411\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 52.2%\n",
      "Minibatch loss at step 200: 1.054441\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 250: 0.594066\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 56.2%\n",
      "Minibatch loss at step 300: 0.850249\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 56.8%\n",
      "Minibatch loss at step 350: 0.692948\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 400: 0.569166\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 56.9%\n",
      "Minibatch loss at step 450: 0.475952\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 56.9%\n",
      "Minibatch loss at step 500: 0.330795\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 56.8%\n",
      "Minibatch loss at step 550: 0.355359\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 57.1%\n",
      "Minibatch loss at step 600: 0.300314\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 56.8%\n",
      "Minibatch loss at step 650: 0.275283\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 57.0%\n",
      "Minibatch loss at step 700: 0.247573\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 55.1%\n",
      "Minibatch loss at step 750: 0.339626\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 56.9%\n",
      "Minibatch loss at step 800: 0.228711\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 56.8%\n",
      "Minibatch loss at step 850: 0.248694\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 57.1%\n",
      "Minibatch loss at step 900: 0.173590\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 56.8%\n",
      "Minibatch loss at step 950: 0.172744\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 56.4%\n",
      "Test accuracy: 57.1%\n",
      "time:  60.65888718735792\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "num_labels = 2\n",
    "#Graph define\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #Input Data placeholder\n",
    "    tf_dataset = tf.placeholder(dtype=tf.float32, shape=[None, feature_size])\n",
    "    tf_labels = tf.placeholder(dtype=tf.float32, shape=[None, num_labels])    \n",
    "    #Hidden layer\n",
    "    tf_hidden_weights = tf.Variable(tf.truncated_normal(dtype=tf.float32, shape=[feature_size, hidden_nodes]))\n",
    "    tf_hidden_bias = tf.Variable(tf.truncated_normal(dtype=tf.float32, shape=[hidden_nodes]))\n",
    "    tf_hidden_in = tf.matmul(tf_dataset, tf_hidden_weights) + tf_hidden_bias\n",
    "    tf_hidden_out =tf.nn.relu(tf_hidden_in)\n",
    "    #Outpu layer\n",
    "    tf_out_weights = tf.Variable(tf.truncated_normal(dtype=tf.float32, shape=[hidden_nodes, num_labels]))\n",
    "    tf_out_bias = tf.Variable(tf.truncated_normal(dtype=tf.float32, shape=[num_labels]))\n",
    "    tf_out_in = tf.matmul(tf_hidden_out, tf_out_weights) + tf_out_bias \n",
    "    #Loss, Optimizer and Predictin\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_labels, logits=tf_out_in))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)    \n",
    "    prediction = tf.nn.softmax(tf_out_in)      \n",
    "    \n",
    "#Train and Test\n",
    "t1 = time.clock()\n",
    "num_steps = 1000\n",
    "train_data_length = len(train_x)\n",
    "with tf.Session(graph = g) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (len(train_y) - batch_size)    \n",
    "        batch_data = train_x[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_y[offset:(offset + batch_size)]\n",
    "        \n",
    "        train_feed = {tf_dataset : batch_data, tf_labels : batch_labels}\n",
    "        #Train\n",
    "        _, train_loss, train_predictions = session.run([optimizer, loss, prediction], feed_dict = train_feed)\n",
    "        if step%50 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, train_loss))            \n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_predictions, batch_labels))\n",
    "            #Valid\n",
    "            valid_feed = {tf_dataset : valid_x, tf_labels : valid_y}\n",
    "            valid_predictions = session.run(prediction, feed_dict = valid_feed)            \n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_predictions, valid_y))\n",
    "    #Test\n",
    "    test_feed = {tf_dataset : test_x, tf_labels : test_y}\n",
    "    test_predictions = session.run(prediction, feed_dict = test_feed)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_predictions, test_y))\n",
    "t2 = time.clock()\n",
    "print('time: ', str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
